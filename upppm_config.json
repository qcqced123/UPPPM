{
    "pipeline_setting": {
        "train": true,
        "test": false,
        "checkpoint_dir": "./saved/model/token_classification/",
        "resume": false,
        "state_dict": "",
        "name": "FBPTrainer",
        "loop": "train_loop",
        "dataset": "UPPPMDataset",
        "model_arch": "TokenModel",
        "model": "microsoft/deberta-v3-large",
        "pooling": "GEMPooling"
    },
    "common_settings": {
        "wandb": true,
        "optuna": false,
        "competition": "FBP3",
        "seed": 42,
        "n_gpu": 1,
        "gpu_id": 0,
        "num_workers": 0
    },
    "data_settings": {
        "n_folds": 5,
        "max_len": 1536,
        "epochs": 20,
        "batch_size": 32
    },
    "gradient_settings": {
        "amp_scaler": true,
        "gradient_checkpoint": true,
        "clipping_grad": true,
        "n_gradient_accumulation_steps": 1,
        "max_grad_norm": 1
    },
    "loss_options": {
        "loss_fn": "BinaryCrossEntropyLoss",
        "val_loss_fn": "pearson_score",
        "reduction": "mean"
    },
    "metrics_options": {
        "metrics": ["WeightMCRMSELoss", "f_beta", "recall"]
    },
    "optimizer_options": {
        "optimizer": "AdamW",
        "llrd": true,
        "layerwise_lr": 5e-6,
        "layerwise_lr_decay": 0.9,
        "layerwise_weight_decay": 1e-2,
        "layerwise_adam_epsilon": 1e-6,
        "layerwise_use_bertadam": false,
        "betas": [0.9, 0.999]
    },
    "scheduler_options": {
        "scheduler": "cosine_annealing",
        "batch_scheduler": true,
        "num_cycles": 2,
        "warmup_ratio": 0.1
    },
    "swa_options": {
        "swa": true,
        "swa_lr": 5e-6,
        "anneal_epochs": 1,
        "anneal_strategy": "cos"
    },
    "model_utils": {
        "stop_mode": "max",
        "freeze": true,
        "num_freeze": 4,
        "reinit": true,
        "num_reinit": 2,
        "awp": false,
        "nth_awp_start_epoch": 0,
        "awp_eps": 1e-2,
        "awp_lr": 1e-4
    }
}
