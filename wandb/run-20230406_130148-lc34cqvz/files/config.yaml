wandb_version: 1

_wandb:
  desc: null
  value:
    cli_version: 0.13.9
    framework: huggingface
    huggingface_version: 4.25.1
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.9.4
    start_time: 1680753708.039998
    t:
      1:
      - 1
      - 5
      - 11
      - 35
      - 49
      - 53
      - 55
      2:
      - 1
      - 5
      - 11
      - 35
      - 49
      - 53
      - 55
      3:
      - 13
      - 16
      - 23
      4: 3.9.4
      5: 0.13.9
      6: 4.25.1
      8:
      - 4
      - 5
amp_scaler:
  desc: null
  value: true
anneal_epochs:
  desc: null
  value: 4
anneal_strategy:
  desc: null
  value: cos
awp:
  desc: null
  value: false
awp_eps:
  desc: null
  value: 0.01
awp_lr:
  desc: null
  value: 0.0001
backbone:
  desc: null
  value: microsoft/deberta-v3-large
batch_scheduler:
  desc: null
  value: true
batch_size:
  desc: null
  value: 32
betas:
  desc: null
  value:
  - 0.9
  - 0.999
cfg_name:
  desc: null
  value: CFG
checkpoint_dir:
  desc: null
  value: ./saved/model
clipping_grad:
  desc: null
  value: true
competition:
  desc: null
  value: FBP3
dataset:
  desc: null
  value: FBPDataset
device:
  desc: null
  value: cpu
epochs:
  desc: null
  value: 5
freeze:
  desc: null
  value: false
gpu_id:
  desc: null
  value: 0
gradient_checkpoint:
  desc: null
  value: true
layerwise_adam_epsilon:
  desc: null
  value: 1.0e-06
layerwise_lr:
  desc: null
  value: 5.0e-05
layerwise_lr_decay:
  desc: null
  value: 0.9
layerwise_use_bertadam:
  desc: null
  value: false
layerwise_weight_decay:
  desc: null
  value: 0.01
llrd:
  desc: null
  value: true
loss_fn:
  desc: null
  value: WeightMCRMSELoss
max_grad_norm:
  desc: null
  value: 1000
max_len:
  desc: null
  value: 1468
metrics:
  desc: null
  value:
  - WeightMCRMSELoss
  - f_beta
  - recall
model_arch:
  desc: null
  value: FBPModel
n_folds:
  desc: null
  value: 5
n_gpu:
  desc: null
  value: 1
n_gradient_accumulation_steps:
  desc: null
  value: 1
name:
  desc: null
  value: FBPTrainer
nth_awp_start_epoch:
  desc: null
  value: 0
num_cycles:
  desc: null
  value: 4
num_grad_norm:
  desc: null
  value: 10
num_reinit:
  desc: null
  value: 5
num_workers:
  desc: null
  value: 0
optimizer:
  desc: null
  value: AdamW
optuna:
  desc: null
  value: false
pooling:
  desc: null
  value: MeanPooling
reduction:
  desc: null
  value: mean
reinit:
  desc: null
  value: true
resume:
  desc: null
  value: false
scheduler:
  desc: null
  value: cosine_annealing
seed:
  desc: null
  value: 42
state_dict:
  desc: null
  value: /
swa:
  desc: null
  value: true
swa_lr:
  desc: null
  value: 0.0001
swa_start:
  desc: null
  value: 135
test:
  desc: null
  value: false
tokenizer:
  desc: null
  value: 'PreTrainedTokenizerFast(name_or_path=''microsoft/deberta-v3-large'', vocab_size=128000,
    model_max_len=1000000000000000019884624838656, is_fast=True, padding_side=''right'',
    truncation_side=''right'', special_tokens={''bos_token'': ''[CLS]'', ''eos_token'':
    ''[SEP]'', ''unk_token'': ''[UNK]'', ''sep_token'': ''[SEP]'', ''pad_token'':
    ''[PAD]'', ''cls_token'': ''[CLS]'', ''mask_token'': ''[MASK]''})'
train:
  desc: null
  value: true
wandb:
  desc: null
  value: true
warmup_ratio:
  desc: null
  value: 0.1
